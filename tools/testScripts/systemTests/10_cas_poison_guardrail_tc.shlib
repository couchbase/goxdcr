# Copyright 2019-Present Couchbase, Inc.
#
# Use of this software is governed by the Business Source License included in
# the file licenses/BSL-Couchbase.txt.  As of the Change Date specified in that
# file, in accordance with the Business Source License, use of this software
# will be governed by the Apache License, Version 2.0, included in the file
# licenses/APL2.txt.

# =============================
# topological map information
# =============================
# cluster -> Bucket(s)
# -----------------
CLUSTER_NAME_PORT_MAP=(["C1"]=9000 ["C2"]=9001)
CLUSTER_NAME_XDCR_PORT_MAP=(["C1"]=13000 ["C2"]=13001)
# Set c1 to have 2 buckets and c2 to have 1 bucket
declare -a cluster1BucketsArr
cluster1BucketsArr=("B0" "B1")
CLUSTER_NAME_BUCKET_MAP=(["C1"]=${cluster1BucketsArr[@]} ["C2"]="B2")

# Bucket properties
declare -A Bucket1Properties=(["ramQuotaMB"]=1500 ["CompressionMode"]="Active")
declare -A Bucket2Properties=(["ramQuotaMB"]=100 ["CompressionMode"]="Active")
insertPropertyIntoBucketNamePropertyMap "B0" Bucket1Properties
insertPropertyIntoBucketNamePropertyMap "B1" Bucket1Properties
insertPropertyIntoBucketNamePropertyMap "B2" Bucket2Properties

declare -A DefaultBucketReplProperties=(["replicationType"]="continuous" ["checkpointInterval"]=60 ["statsInterval"]=500)

# Bucket -> Scopes
# -----------------
declare -a scope1Arr=("S1" "S2")
BUCKET_NAME_SCOPE_MAP=(["B1"]=${scope1Arr[@]} ["B2"]="S1")

# Scopes -> Collections
# ----------------------
declare -a collection1Arr=("col1" "col2")
declare -a collection2Arr=("col1" "col2" "col3")
SCOPE_NAME_COLLECTION_MAP=(["S1"]=${collection1Arr[@]} ["S2"]=${collection2Arr[@]} ["S3"]=${collection2Arr[@]})

function runDataLoad {
	echo "RUNNING dataload..."
	# Cbworkload gen docs seem to have the following content:
	# {
	#  "name": "xdcrProv_C11000000",
	#  "age": 100,
	#  "index": "1000000",
	#  "body": "0000000000"
	#}
	# which is about 98 bytes
	# Round it down to about 80 bytes per doc
	# 100MB = 100000000 bytes
	# Need 1250000 documents
	echo ""
	# For now, just is diag eval so no need to load the actual size
	runCbWorkloadGenBucket "C1" "B1" 12500
}

function runTestCase {
	echo "============================================================================"
	echo "Running live CAS poisoning guardrail test case"
	echo "============================================================================"
	setupTopologies
	if (($? != 0)); then
		exit 1
	fi

	# Wait for vbuckets and all the other things to propagate before XDCR provisioning
	sleep 5
	createRemoteClusterReference "C1" "C2"
	createRemoteClusterReference "C2" "C1"
	sleep 1

	createBucketReplication "C1" "B1" "C2" "B2" DefaultBucketReplProperties
	createBucketReplication "C2" "B2" "C1" "B1" DefaultBucketReplProperties

	setReplicationSettings "C1" "B1" "C2" "B2" "xdcrDevCasDriftInjectDocKey=xdcrProv_C1100"

	sleep 20
	runDataLoad

	sleep 15
	validatePrometheusStatsCasPoisonNon0 "C1" "B1" "B2"

	waitForNumberOfEvents "C1" 1 1
	checkBidirectionalChangesLeft

	pauseReplication "C1" "B1" "C2" "B2"
	sleep 30
	killGoXdcr "C1"
	sleep 15
	resumeReplication "C1" "B1" "C2" "B2"
	sleep 20
	validatePrometheusStatsCasPoisonNon0 "C1" "B1" "B2"

	echo "============================================================================"
	echo "PASSED"
	echo "============================================================================"
	cleanupBucketReplications
	cleanupBuckets
	cleanupRemoteClusterRefs
}
